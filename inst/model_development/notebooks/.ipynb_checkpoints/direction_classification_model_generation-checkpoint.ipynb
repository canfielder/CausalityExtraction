{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "# Analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from siuba import *\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy.stats import loguniform, uniform\n",
    "\n",
    "# Modeling\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import ComplementNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Custom\n",
    "from source_causality_direction import *\n",
    "\n",
    "# Downloads\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "# uploaded=files.upload()\n",
    "dataset=pd.read_excel(\"./../data/Outputs/training_data_dir_multiclass.xlsx\", engine=\"openpyxl\")\n",
    "\n",
    "#convert into lists\n",
    "df=pd.DataFrame(\n",
    "    {\"causality\": dataset.causal_relationship,\n",
    "     \"direction\": dataset.direction, \n",
    "     \"text\": dataset.sentence, \n",
    "     \"node1\": dataset.node_1, \n",
    "     \"node2\":dataset.node_2}\n",
    ")                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Train/Test split\n",
    "test_size=0.25\n",
    "\n",
    "# Define Random State\n",
    "rs=5590\n",
    "\n",
    "# Define output directory\n",
    "directory=\"./../data/Outputs/models/causality_direction/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models\n",
    "We will evaluate the following different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dag-of-Words Feature Processing\n",
    "dictio_models_bow={\n",
    "    \"Logistic Regression\": \n",
    "        LogisticRegression(C=1e5, max_iter=1000, random_state=rs),\n",
    "    \"Naive Bayes, Multinomial\": \n",
    "        MultinomialNB(),\n",
    "    \"Naive Bayes, Complement\": \n",
    "        ComplementNB(),\n",
    "    \"Random Forest\": \n",
    "        RandomForestClassifier(random_state=rs),\n",
    "    \"Support Vector Machines\": \n",
    "        SVC(kernel=\"linear\", random_state=rs)\n",
    "}\n",
    "\n",
    "# Doc2Vec Feature Processing\n",
    "dictio_models_doc2vec={\n",
    "    \"Logistic Regression\": \n",
    "        LogisticRegression(C=1e5, max_iter=1000, random_state=rs),\n",
    "    \"Random Forest\": \n",
    "        RandomForestClassifier(random_state=rs),\n",
    "    \"Support Vector Machines\": \n",
    "        SVC(kernel=\"linear\", random_state=rs),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Iterator Inputs\n",
    "The following are the differen modeling and processing steps we will evaluate. We will create a list of all posssible combinations, and then iterate through them, evaluating the model performance for each configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets=[\"causality\", \"direction\"]\n",
    "methods=[\"lemm\", \"stem\"]\n",
    "entities=[True, False]\n",
    "balance_methods=[\"none\", \"smote\", \"ada\"]\n",
    "\n",
    "conditions=list(itertools.product(targets, methods, entities, balance_methods))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\py-CausalityExtraction_36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda3\\envs\\py-CausalityExtraction_36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda3\\envs\\py-CausalityExtraction_36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda3\\envs\\py-CausalityExtraction_36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda3\\envs\\py-CausalityExtraction_36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda3\\envs\\py-CausalityExtraction_36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda3\\envs\\py-CausalityExtraction_36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda3\\envs\\py-CausalityExtraction_36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda3\\envs\\py-CausalityExtraction_36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda3\\envs\\py-CausalityExtraction_36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda3\\envs\\py-CausalityExtraction_36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda3\\envs\\py-CausalityExtraction_36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda3\\envs\\py-CausalityExtraction_36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda3\\envs\\py-CausalityExtraction_36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda3\\envs\\py-CausalityExtraction_36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda3\\envs\\py-CausalityExtraction_36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda3\\envs\\py-CausalityExtraction_36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda3\\envs\\py-CausalityExtraction_36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists\n",
    "lst_target=[]\n",
    "lst_norm=[]\n",
    "lst_feature_string=[]\n",
    "lst_entity=[]\n",
    "lst_balance=[]\n",
    "lst_feature=[]\n",
    "lst_model=[]\n",
    "lst_acc=[]\n",
    "lst_precision_macro=[]\n",
    "lst_recall_macro=[]\n",
    "lst_f1_macro=[]\n",
    "\n",
    "for idx, alignment in enumerate(conditions):\n",
    "    # Extract / Report Iteration Alignment ------------------------------------\n",
    "    TARGET=alignment[0]\n",
    "    TOKENIZATION_METHOD=alignment[1]\n",
    "    ENTITY_REPLACEMENT=alignment[2]\n",
    "    BALANCE=alignment[3]\n",
    "    \n",
    "#     print(f\"ITERATION: {idx + 1}\")\n",
    "#     print(f\"Target Variable:\\t\\t{TARGET}\")\n",
    "#     print(f\"Text Normalization Method:\\t{TOKENIZATION_METHOD}\")\n",
    "#     print(f\"Entity Node Replacement:\\t{ENTITY_REPLACEMENT}\")\n",
    "#     print(f\"Sythetic Balancing:\\t\\t{BALANCE}\")\n",
    "#     print()\n",
    "    \n",
    "    # Process Model Input -----------------------------------------------------\n",
    "    df_input=df.copy()\n",
    "    y, X=gen_target_features(\n",
    "        df_input, \n",
    "        TARGET, \n",
    "        token_method=TOKENIZATION_METHOD, \n",
    "        balance=BALANCE,\n",
    "        entity_replacement=ENTITY_REPLACEMENT\n",
    "    )\n",
    "    \n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test=train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size, \n",
    "        random_state=rs\n",
    "    )\n",
    "       \n",
    "    \n",
    "    #Doc2Vec Preparation\n",
    "    corpus_train=list(read_corpus(X_train))\n",
    "    corpus_test=list(read_corpus(X_test))\n",
    "    \n",
    "    # Initialize Model\n",
    "    model=gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "\n",
    "    # Build Vocab\n",
    "    model.build_vocab(corpus_train)\n",
    "\n",
    "    # Train model\n",
    "    model.train( \n",
    "        corpus_train,\n",
    "        total_examples=model.corpus_count, \n",
    "        epochs=model.epochs\n",
    "    )\n",
    "    \n",
    "    X_train_doc2vec=doc2vec_xfrm(X_train, model)\n",
    "    X_test_doc2vec=doc2vec_xfrm(X_test, model)\n",
    "    \n",
    "    # Initialize Model Inputs -------------------------------------------------\n",
    "    dictio_input={\n",
    "    \"Doc2Vec\": {\n",
    "        \"data\": {\n",
    "            \"X_train\": X_train_doc2vec,\n",
    "            \"X_test\": X_test_doc2vec,\n",
    "            \"y_train\": y_train,\n",
    "            \"y_test\": y_test,\n",
    "        },\n",
    "        \"models\": dictio_models_doc2vec,\n",
    "    },\n",
    "    \"BOW\": {\n",
    "        \"data\": {\n",
    "            \"X_train\": X_train,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_train\": y_train,\n",
    "            \"y_test\": y_test,\n",
    "        },\n",
    "        \"models\": dictio_models_bow,\n",
    "    }\n",
    "    }\n",
    "    \n",
    "    # Execute Model Training --------------------------------------------------\n",
    "    dictio_alignment={}\n",
    "    for feature_type, values in dictio_input.items():\n",
    "#         print(f\"Feature Processing: {feature_type}\")\n",
    "    \n",
    "        X_train=values[\"data\"][\"X_train\"]\n",
    "        X_test=values[\"data\"][\"X_test\"]\n",
    "        y_train=values[\"data\"][\"y_train\"]\n",
    "        y_test=values[\"data\"][\"y_test\"]\n",
    "    \n",
    "        dictio_model_output={}\n",
    "        for model_label, clf in values[\"models\"].items():\n",
    "            if feature_type == \"BOW\":\n",
    "                clf=gen_pipeline(clf, BALANCE, rs)\n",
    "        \n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred=clf.predict(X_test)\n",
    "            acc=accuracy_score(y_test, y_pred)\n",
    "            prfs_macro=precision_recall_fscore_support(y_test, y_pred, average=\"macro\")\n",
    "#             print(f\"Model Type: {model_label}\")\n",
    "#             print(classification_report(y_test, y_pred))\n",
    "#             print()\n",
    "            \n",
    "            \n",
    "            # Save to Output Lists\n",
    "            lst_target.append(TARGET)\n",
    "            lst_norm.append(TOKENIZATION_METHOD)\n",
    "            lst_entity.append(ENTITY_REPLACEMENT)\n",
    "            lst_balance.append(BALANCE)\n",
    "            lst_feature.append(feature_type)\n",
    "            lst_model.append(model_label)\n",
    "            lst_acc.append(acc)\n",
    "            lst_precision_macro.append(prfs_macro[0])\n",
    "            lst_recall_macro.append(prfs_macro[1])\n",
    "            lst_f1_macro.append(prfs_macro[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results Table\n",
    "After training and evaluating every configuration alignment, we export the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>token_normalization</th>\n",
       "      <th>entity_replacement</th>\n",
       "      <th>balance</th>\n",
       "      <th>feature_method</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>causality</td>\n",
       "      <td>stem</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>BOW</td>\n",
       "      <td>Support Vector Machines</td>\n",
       "      <td>0.930818</td>\n",
       "      <td>0.924854</td>\n",
       "      <td>0.909065</td>\n",
       "      <td>0.916416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>direction</td>\n",
       "      <td>lemm</td>\n",
       "      <td>False</td>\n",
       "      <td>ada</td>\n",
       "      <td>BOW</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.746190</td>\n",
       "      <td>0.791711</td>\n",
       "      <td>0.765660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>direction</td>\n",
       "      <td>lemm</td>\n",
       "      <td>False</td>\n",
       "      <td>smote</td>\n",
       "      <td>BOW</td>\n",
       "      <td>Naive Bayes, Complement</td>\n",
       "      <td>0.757764</td>\n",
       "      <td>0.641849</td>\n",
       "      <td>0.679281</td>\n",
       "      <td>0.650827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>direction</td>\n",
       "      <td>lemm</td>\n",
       "      <td>False</td>\n",
       "      <td>ada</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.720497</td>\n",
       "      <td>0.582251</td>\n",
       "      <td>0.336622</td>\n",
       "      <td>0.308608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>causality</td>\n",
       "      <td>stem</td>\n",
       "      <td>True</td>\n",
       "      <td>smote</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.823899</td>\n",
       "      <td>0.862037</td>\n",
       "      <td>0.720158</td>\n",
       "      <td>0.748645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        target token_normalization  entity_replacement balance feature_method  \\\n",
       "79   causality                stem               False    none            BOW   \n",
       "139  direction                lemm               False     ada            BOW   \n",
       "133  direction                lemm               False   smote            BOW   \n",
       "137  direction                lemm               False     ada        Doc2Vec   \n",
       "57   causality                stem                True   smote        Doc2Vec   \n",
       "\n",
       "                       model  accuracy  precision_macro  recall_macro  \\\n",
       "79   Support Vector Machines  0.930818         0.924854      0.909065   \n",
       "139      Logistic Regression  0.857143         0.746190      0.791711   \n",
       "133  Naive Bayes, Complement  0.757764         0.641849      0.679281   \n",
       "137            Random Forest  0.720497         0.582251      0.336622   \n",
       "57             Random Forest  0.823899         0.862037      0.720158   \n",
       "\n",
       "     f1_macro  \n",
       "79   0.916416  \n",
       "139  0.765660  \n",
       "133  0.650827  \n",
       "137  0.308608  \n",
       "57   0.748645  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results=pd.DataFrame({\n",
    "    \"target\": lst_target,\n",
    "    \"token_normalization\": lst_norm, \n",
    "    \"entity_replacement\": lst_entity, \n",
    "    \"balance\": lst_balance,\n",
    "    \"feature_method\": lst_feature,\n",
    "    \"model\": lst_model,\n",
    "    \"accuracy\": lst_acc,\n",
    "    \"precision_macro\": lst_precision_macro,\n",
    "    \"recall_macro\": lst_recall_macro,\n",
    "    \"f1_macro\": lst_f1_macro\n",
    "})   \n",
    "\n",
    "file_name=\"causality_direction_evaluation.csv\"\n",
    "path_result_summary=os.path.join(directory, file_name)\n",
    "df_results.to_csv(path_result_summary, index=False)\n",
    "df_results.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Table\n",
    "We will take the complete results table and create a summary table, which lists the top alignments, by f1-score, for each target, features-type, balance, and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>token_normalization</th>\n",
       "      <th>entity_replacement</th>\n",
       "      <th>balance</th>\n",
       "      <th>feature_method</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_macro_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>causality</td>\n",
       "      <td>lemm</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>Support Vector Machines</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.349057</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>causality</td>\n",
       "      <td>lemm</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>BOW</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.949686</td>\n",
       "      <td>0.951186</td>\n",
       "      <td>0.928491</td>\n",
       "      <td>0.938823</td>\n",
       "      <td>0.938823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>causality</td>\n",
       "      <td>lemm</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>BOW</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.937107</td>\n",
       "      <td>0.949685</td>\n",
       "      <td>0.901745</td>\n",
       "      <td>0.921443</td>\n",
       "      <td>0.921443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>causality</td>\n",
       "      <td>lemm</td>\n",
       "      <td>True</td>\n",
       "      <td>smote</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>Support Vector Machines</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.349057</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>causality</td>\n",
       "      <td>lemm</td>\n",
       "      <td>True</td>\n",
       "      <td>smote</td>\n",
       "      <td>BOW</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.911950</td>\n",
       "      <td>0.895552</td>\n",
       "      <td>0.895552</td>\n",
       "      <td>0.895552</td>\n",
       "      <td>0.895552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>direction</td>\n",
       "      <td>stem</td>\n",
       "      <td>False</td>\n",
       "      <td>ada</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>Support Vector Machines</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.246377</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.283333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>direction</td>\n",
       "      <td>stem</td>\n",
       "      <td>False</td>\n",
       "      <td>ada</td>\n",
       "      <td>BOW</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.768452</td>\n",
       "      <td>0.823747</td>\n",
       "      <td>0.791394</td>\n",
       "      <td>0.791394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>direction</td>\n",
       "      <td>stem</td>\n",
       "      <td>False</td>\n",
       "      <td>ada</td>\n",
       "      <td>BOW</td>\n",
       "      <td>Naive Bayes, Multinomial</td>\n",
       "      <td>0.788820</td>\n",
       "      <td>0.700240</td>\n",
       "      <td>0.722772</td>\n",
       "      <td>0.698385</td>\n",
       "      <td>0.698385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>direction</td>\n",
       "      <td>stem</td>\n",
       "      <td>False</td>\n",
       "      <td>ada</td>\n",
       "      <td>BOW</td>\n",
       "      <td>Naive Bayes, Complement</td>\n",
       "      <td>0.776398</td>\n",
       "      <td>0.686404</td>\n",
       "      <td>0.705479</td>\n",
       "      <td>0.677714</td>\n",
       "      <td>0.677714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>direction</td>\n",
       "      <td>stem</td>\n",
       "      <td>False</td>\n",
       "      <td>ada</td>\n",
       "      <td>BOW</td>\n",
       "      <td>Support Vector Machines</td>\n",
       "      <td>0.832298</td>\n",
       "      <td>0.711756</td>\n",
       "      <td>0.765764</td>\n",
       "      <td>0.732306</td>\n",
       "      <td>0.732306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target token_normalization  entity_replacement balance feature_method  \\\n",
       "2    causality                lemm                True    none        Doc2Vec   \n",
       "3    causality                lemm                True    none            BOW   \n",
       "6    causality                lemm                True    none            BOW   \n",
       "10   causality                lemm                True   smote        Doc2Vec   \n",
       "11   causality                lemm                True   smote            BOW   \n",
       "..         ...                 ...                 ...     ...            ...   \n",
       "186  direction                stem               False     ada        Doc2Vec   \n",
       "187  direction                stem               False     ada            BOW   \n",
       "188  direction                stem               False     ada            BOW   \n",
       "189  direction                stem               False     ada            BOW   \n",
       "191  direction                stem               False     ada            BOW   \n",
       "\n",
       "                        model  accuracy  precision_macro  recall_macro  \\\n",
       "2     Support Vector Machines  0.698113         0.349057      0.500000   \n",
       "3         Logistic Regression  0.949686         0.951186      0.928491   \n",
       "6               Random Forest  0.937107         0.949685      0.901745   \n",
       "10    Support Vector Machines  0.698113         0.349057      0.500000   \n",
       "11        Logistic Regression  0.911950         0.895552      0.895552   \n",
       "..                        ...       ...              ...           ...   \n",
       "186   Support Vector Machines  0.739130         0.246377      0.333333   \n",
       "187       Logistic Regression  0.869565         0.768452      0.823747   \n",
       "188  Naive Bayes, Multinomial  0.788820         0.700240      0.722772   \n",
       "189   Naive Bayes, Complement  0.776398         0.686404      0.705479   \n",
       "191   Support Vector Machines  0.832298         0.711756      0.765764   \n",
       "\n",
       "     f1_macro  f1_macro_max  \n",
       "2    0.411111      0.411111  \n",
       "3    0.938823      0.938823  \n",
       "6    0.921443      0.921443  \n",
       "10   0.411111      0.411111  \n",
       "11   0.895552      0.895552  \n",
       "..        ...           ...  \n",
       "186  0.283333      0.283333  \n",
       "187  0.791394      0.791394  \n",
       "188  0.698385      0.698385  \n",
       "189  0.677714      0.677714  \n",
       "191  0.732306      0.732306  \n",
       "\n",
       "[109 rows x 11 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary = (df_results\n",
    " >> group_by('target', 'entity_replacement', 'feature_method', 'balance', 'model')\n",
    " >> mutate(\n",
    "     f1_macro_max = _.f1_macro.max()\n",
    " )\n",
    " >> ungroup()\n",
    " >> filter(\n",
    "     _.f1_macro == _.f1_macro_max,\n",
    " )\n",
    "#  >> select(-_.f1_macro_max)\n",
    ")\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direction\n",
    "* Due to poor performance of entity extraction in external datasets, we will limit Direction classification to only cases without entity extraction.\n",
    "* Oversampling methods showed very small improvement, but not enough to justify use of sythetic data. \n",
    "* BOW feature generation greatly outperfromed Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>token_normalization</th>\n",
       "      <th>entity_replacement</th>\n",
       "      <th>balance</th>\n",
       "      <th>feature_method</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_macro_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>direction</td>\n",
       "      <td>stem</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>BOW</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.857436</td>\n",
       "      <td>0.727163</td>\n",
       "      <td>0.768443</td>\n",
       "      <td>0.768443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>direction</td>\n",
       "      <td>stem</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>BOW</td>\n",
       "      <td>Naive Bayes, Complement</td>\n",
       "      <td>0.813665</td>\n",
       "      <td>0.721055</td>\n",
       "      <td>0.704491</td>\n",
       "      <td>0.705928</td>\n",
       "      <td>0.705928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>direction</td>\n",
       "      <td>stem</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>BOW</td>\n",
       "      <td>Naive Bayes, Multinomial</td>\n",
       "      <td>0.801242</td>\n",
       "      <td>0.724246</td>\n",
       "      <td>0.637380</td>\n",
       "      <td>0.663053</td>\n",
       "      <td>0.663053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>direction</td>\n",
       "      <td>stem</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>BOW</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.892515</td>\n",
       "      <td>0.674794</td>\n",
       "      <td>0.719502</td>\n",
       "      <td>0.719502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>direction</td>\n",
       "      <td>stem</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>BOW</td>\n",
       "      <td>Support Vector Machines</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.809114</td>\n",
       "      <td>0.706818</td>\n",
       "      <td>0.744160</td>\n",
       "      <td>0.744160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        target token_normalization  entity_replacement balance feature_method  \\\n",
       "171  direction                stem               False    none            BOW   \n",
       "173  direction                stem               False    none            BOW   \n",
       "172  direction                stem               False    none            BOW   \n",
       "174  direction                stem               False    none            BOW   \n",
       "175  direction                stem               False    none            BOW   \n",
       "\n",
       "                        model  accuracy  precision_macro  recall_macro  \\\n",
       "171       Logistic Regression  0.869565         0.857436      0.727163   \n",
       "173   Naive Bayes, Complement  0.813665         0.721055      0.704491   \n",
       "172  Naive Bayes, Multinomial  0.801242         0.724246      0.637380   \n",
       "174             Random Forest  0.857143         0.892515      0.674794   \n",
       "175   Support Vector Machines  0.857143         0.809114      0.706818   \n",
       "\n",
       "     f1_macro  f1_macro_max  \n",
       "171  0.768443      0.768443  \n",
       "173  0.705928      0.705928  \n",
       "172  0.663053      0.663053  \n",
       "174  0.719502      0.719502  \n",
       "175  0.744160      0.744160  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_summary\n",
    " >> filter(\n",
    "     _.target ==\"direction\",\n",
    "     _.entity_replacement == False,\n",
    "     _.balance == \"none\",\n",
    "     _.feature_method == \"BOW\",\n",
    " )\n",
    " >> arrange(_.model)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>token_normalization</th>\n",
       "      <th>entity_replacement</th>\n",
       "      <th>balance</th>\n",
       "      <th>feature_method</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_macro_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>direction</td>\n",
       "      <td>lemm</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.720497</td>\n",
       "      <td>0.514570</td>\n",
       "      <td>0.445410</td>\n",
       "      <td>0.458310</td>\n",
       "      <td>0.458310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>direction</td>\n",
       "      <td>stem</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.356717</td>\n",
       "      <td>0.336970</td>\n",
       "      <td>0.336970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>direction</td>\n",
       "      <td>lemm</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>Support Vector Machines</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.246377</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.283333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>direction</td>\n",
       "      <td>stem</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>Support Vector Machines</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.246377</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.283333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        target token_normalization  entity_replacement balance feature_method  \\\n",
       "120  direction                lemm               False    none        Doc2Vec   \n",
       "169  direction                stem               False    none        Doc2Vec   \n",
       "122  direction                lemm               False    none        Doc2Vec   \n",
       "170  direction                stem               False    none        Doc2Vec   \n",
       "\n",
       "                       model  accuracy  precision_macro  recall_macro  \\\n",
       "120      Logistic Regression  0.720497         0.514570      0.445410   \n",
       "169            Random Forest  0.739130         0.583333      0.356717   \n",
       "122  Support Vector Machines  0.739130         0.246377      0.333333   \n",
       "170  Support Vector Machines  0.739130         0.246377      0.333333   \n",
       "\n",
       "     f1_macro  f1_macro_max  \n",
       "120  0.458310      0.458310  \n",
       "169  0.336970      0.336970  \n",
       "122  0.283333      0.283333  \n",
       "170  0.283333      0.283333  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_summary\n",
    " >> filter(\n",
    "     _.target ==\"direction\",\n",
    "     _.entity_replacement == False,\n",
    "     _.balance == \"none\",\n",
    "     _.feature_method == \"Doc2Vec\",\n",
    " )\n",
    " >> arrange(_.model)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causality\n",
    "* Due to poor performance of entity extraction in external datasets, we will limit Direction classification to only cases without entity extraction.\n",
    "* Oversampling methods showed very small improvement, but not enough to justify use of sythetic data. \n",
    "* BOW feature generation greatly outperfromed Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>token_normalization</th>\n",
       "      <th>entity_replacement</th>\n",
       "      <th>balance</th>\n",
       "      <th>feature_method</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_macro_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>causality</td>\n",
       "      <td>stem</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>BOW</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.937107</td>\n",
       "      <td>0.935474</td>\n",
       "      <td>0.913570</td>\n",
       "      <td>0.923528</td>\n",
       "      <td>0.923528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>causality</td>\n",
       "      <td>stem</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>BOW</td>\n",
       "      <td>Naive Bayes, Complement</td>\n",
       "      <td>0.918239</td>\n",
       "      <td>0.909357</td>\n",
       "      <td>0.894144</td>\n",
       "      <td>0.901219</td>\n",
       "      <td>0.901219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>causality</td>\n",
       "      <td>lemm</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>BOW</td>\n",
       "      <td>Naive Bayes, Multinomial</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>0.893860</td>\n",
       "      <td>0.879223</td>\n",
       "      <td>0.886022</td>\n",
       "      <td>0.886022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>causality</td>\n",
       "      <td>lemm</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>BOW</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>0.940476</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.875762</td>\n",
       "      <td>0.875762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>causality</td>\n",
       "      <td>stem</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>BOW</td>\n",
       "      <td>Support Vector Machines</td>\n",
       "      <td>0.930818</td>\n",
       "      <td>0.924854</td>\n",
       "      <td>0.909065</td>\n",
       "      <td>0.916416</td>\n",
       "      <td>0.916416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       target token_normalization  entity_replacement balance feature_method  \\\n",
       "75  causality                stem               False    none            BOW   \n",
       "77  causality                stem               False    none            BOW   \n",
       "28  causality                lemm               False    none            BOW   \n",
       "30  causality                lemm               False    none            BOW   \n",
       "79  causality                stem               False    none            BOW   \n",
       "\n",
       "                       model  accuracy  precision_macro  recall_macro  \\\n",
       "75       Logistic Regression  0.937107         0.935474      0.913570   \n",
       "77   Naive Bayes, Complement  0.918239         0.909357      0.894144   \n",
       "28  Naive Bayes, Multinomial  0.905660         0.893860      0.879223   \n",
       "30             Random Forest  0.905660         0.940476      0.843750   \n",
       "79   Support Vector Machines  0.930818         0.924854      0.909065   \n",
       "\n",
       "    f1_macro  f1_macro_max  \n",
       "75  0.923528      0.923528  \n",
       "77  0.901219      0.901219  \n",
       "28  0.886022      0.886022  \n",
       "30  0.875762      0.875762  \n",
       "79  0.916416      0.916416  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_summary\n",
    " >> filter(\n",
    "     _.target ==\"causality\",\n",
    "     _.entity_replacement == False,\n",
    "     _.balance == \"none\",\n",
    "     _.feature_method == \"BOW\",\n",
    " )\n",
    " >> arrange(_.model)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>token_normalization</th>\n",
       "      <th>entity_replacement</th>\n",
       "      <th>balance</th>\n",
       "      <th>feature_method</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_macro_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>causality</td>\n",
       "      <td>lemm</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.761006</td>\n",
       "      <td>0.716616</td>\n",
       "      <td>0.686937</td>\n",
       "      <td>0.697173</td>\n",
       "      <td>0.697173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>causality</td>\n",
       "      <td>stem</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.792453</td>\n",
       "      <td>0.795522</td>\n",
       "      <td>0.685811</td>\n",
       "      <td>0.706626</td>\n",
       "      <td>0.706626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>causality</td>\n",
       "      <td>lemm</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>Support Vector Machines</td>\n",
       "      <td>0.704403</td>\n",
       "      <td>0.851266</td>\n",
       "      <td>0.510417</td>\n",
       "      <td>0.433048</td>\n",
       "      <td>0.433048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>causality</td>\n",
       "      <td>stem</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>Support Vector Machines</td>\n",
       "      <td>0.704403</td>\n",
       "      <td>0.851266</td>\n",
       "      <td>0.510417</td>\n",
       "      <td>0.433048</td>\n",
       "      <td>0.433048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       target token_normalization  entity_replacement balance feature_method  \\\n",
       "24  causality                lemm               False    none        Doc2Vec   \n",
       "73  causality                stem               False    none        Doc2Vec   \n",
       "26  causality                lemm               False    none        Doc2Vec   \n",
       "74  causality                stem               False    none        Doc2Vec   \n",
       "\n",
       "                      model  accuracy  precision_macro  recall_macro  \\\n",
       "24      Logistic Regression  0.761006         0.716616      0.686937   \n",
       "73            Random Forest  0.792453         0.795522      0.685811   \n",
       "26  Support Vector Machines  0.704403         0.851266      0.510417   \n",
       "74  Support Vector Machines  0.704403         0.851266      0.510417   \n",
       "\n",
       "    f1_macro  f1_macro_max  \n",
       "24  0.697173      0.697173  \n",
       "73  0.706626      0.706626  \n",
       "26  0.433048      0.433048  \n",
       "74  0.433048      0.433048  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_summary\n",
    " >> filter(\n",
    "     _.target ==\"causality\",\n",
    "     _.entity_replacement == False,\n",
    "     _.balance == \"none\",\n",
    "     _.feature_method == \"Doc2Vec\",\n",
    " )\n",
    " >> arrange(_.model)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "After the gridsearch-like method of evaluating each configuration, we will perform further hyperparameter tuning on the best models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causality\n",
    "* **Optimal Model**: Logistic Regression\n",
    "* **Optimal Token Normalization**: Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Variable:\t\tcausality\n",
      "Text Normalization Method:\tstem\n",
      "Entity Node Replacement:\tFalse\n",
      "Sythetic Balancing:\t\tnone\n"
     ]
    }
   ],
   "source": [
    "TARGET=targets[0]\n",
    "TOKENIZATION_METHOD=methods[1]\n",
    "ENTITY_REPLACEMENT=entities[1]\n",
    "BALANCE=balance_methods[0]\n",
    "\n",
    "print(f\"Target Variable:\\t\\t{TARGET}\")\n",
    "print(f\"Text Normalization Method:\\t{TOKENIZATION_METHOD}\")\n",
    "print(f\"Entity Node Replacement:\\t{ENTITY_REPLACEMENT}\")\n",
    "print(f\"Sythetic Balancing:\\t\\t{BALANCE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model and Search Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "clf=LogisticRegression(random_state=rs)\n",
    "\n",
    "# Evaluation\n",
    "cv=RepeatedStratifiedKFold(\n",
    "    n_splits=10, \n",
    "    n_repeats=3, \n",
    "    random_state=rs\n",
    ")\n",
    "\n",
    "# Define Search Space\n",
    "param_space = {\n",
    "    'clf__solver': ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "    'clf__penalty':  ['none', 'l1', 'l2', 'elasticnet'],\n",
    "    'clf__C': loguniform(1e-5, 100) \n",
    "}\n",
    "\n",
    "# Generate Pipeline\n",
    "clf=gen_pipeline(\n",
    "    clf=clf, \n",
    "    balance=BALANCE,\n",
    "    random_state=rs\n",
    ")\n",
    "\n",
    "# Define Random Search Parameters\n",
    "random_search=RandomizedSearchCV(\n",
    "    clf, \n",
    "    param_space, \n",
    "    n_iter=500, \n",
    "    scoring='f1_macro', \n",
    "    n_jobs=-1, \n",
    "    verbose=1,\n",
    "    cv=cv, \n",
    "    random_state=rs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process/Target Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input=df.copy()\n",
    "y, X=gen_target_features(\n",
    "    df_input, \n",
    "    TARGET, \n",
    "    token_method=TOKENIZATION_METHOD, \n",
    "    balance=BALANCE,\n",
    "    entity_replacement=ENTITY_REPLACEMENT\n",
    ")\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test=train_test_split(\n",
    "    X, y, \n",
    "    test_size=test_size, \n",
    "    random_state=rs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 30 folds for each of 500 candidates, totalling 15000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 656 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1656 tasks      | elapsed:   18.0s\n",
      "[Parallel(n_jobs=-1)]: Done 3056 tasks      | elapsed:   36.6s\n",
      "[Parallel(n_jobs=-1)]: Done 4856 tasks      | elapsed:   58.7s\n",
      "[Parallel(n_jobs=-1)]: Done 7056 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 9656 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 12656 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 14985 out of 15000 | elapsed:  3.1min remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 15000 out of 15000 | elapsed:  3.1min finished\n",
      "D:\\anaconda3\\envs\\py-CausalityExtraction_36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    }
   ],
   "source": [
    "result=random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.894\n",
      "Best Hyperparameters: {'clf__C': 0.013395544446281223, 'clf__penalty': 'none', 'clf__solver': 'lbfgs'}\n",
      "Pipeline(steps=[('vectorizer',\n",
      "                 CountVectorizer(binary=True, ngram_range=(1, 3))),\n",
      "                ('clf',\n",
      "                 LogisticRegression(C=0.013395544446281223, penalty='none',\n",
      "                                    random_state=5590))])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Score: {result.best_score_:.3}\")\n",
    "print(f\"Best Hyperparameters: {result.best_params_}\")\n",
    "print(result.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.97      0.96       111\n",
      "         1.0       0.93      0.85      0.89        48\n",
      "\n",
      "    accuracy                           0.94       159\n",
      "   macro avg       0.94      0.91      0.92       159\n",
      "weighted avg       0.94      0.94      0.94       159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf=result.best_estimator_\n",
    "y_pred=clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t93.7%\n",
      "Precision:\t93.5%\n",
      "Recall:\t\t91.4%\n",
      "F1-Score:\t92.4%\n"
     ]
    }
   ],
   "source": [
    "acc=accuracy_score(y_test, y_pred)\n",
    "prfs_macro=precision_recall_fscore_support(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy:\\t{acc*100:.1f}%\")\n",
    "print(f\"Precision:\\t{prfs_macro[0]*100:.1f}%\")\n",
    "print(f\"Recall:\\t\\t{prfs_macro[1]*100:.1f}%\")\n",
    "print(f\"F1-Score:\\t{prfs_macro[2]*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./../data/Outputs/models/causality_direction/causality.joblib']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Data\n",
    "df_test=pd.DataFrame({\n",
    "    \"target\": y_test,\n",
    "    \"features\": X_test\n",
    "    })\n",
    "\n",
    "file_test_data=f\"test_data_{TARGET}.csv\"\n",
    "path_test_data=os.path.join(directory, file_test_data)\n",
    "df_test.to_csv(path_test_data, index=False)\n",
    "\n",
    "# Model\n",
    "clf=result.best_estimator_\n",
    "model_name=f\"{TARGET}.joblib\"\n",
    "path=os.path.join(directory, model_name)\n",
    "\n",
    "dump(clf, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direction\n",
    "* **Optimal Model**: Logistic Regression\n",
    "* **Optimal Token Normalization**: Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Variable:\t\tdirection\n",
      "Text Normalization Method:\tstem\n",
      "Entity Node Replacement:\tFalse\n",
      "Sythetic Balancing:\t\tnone\n"
     ]
    }
   ],
   "source": [
    "TARGET=targets[1]\n",
    "TOKENIZATION_METHOD=methods[1]\n",
    "ENTITY_REPLACEMENT=entities[1]\n",
    "BALANCE=balance_methods[0]\n",
    "\n",
    "print(f\"Target Variable:\\t\\t{TARGET}\")\n",
    "print(f\"Text Normalization Method:\\t{TOKENIZATION_METHOD}\")\n",
    "print(f\"Entity Node Replacement:\\t{ENTITY_REPLACEMENT}\")\n",
    "print(f\"Sythetic Balancing:\\t\\t{BALANCE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model and Search Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "clf=LogisticRegression(random_state=rs)\n",
    "\n",
    "# Evaluation\n",
    "cv=RepeatedStratifiedKFold(\n",
    "    n_splits=10, \n",
    "    n_repeats=3, \n",
    "    random_state=rs\n",
    ")\n",
    "\n",
    "# Define Search Space\n",
    "param_space = {\n",
    "    'clf__solver': ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "    'clf__penalty':  ['none', 'l1', 'l2', 'elasticnet'],\n",
    "    'clf__C': loguniform(1e-5, 100) \n",
    "}\n",
    "\n",
    "# Generate Pipeline\n",
    "clf=gen_pipeline(\n",
    "    clf=clf, \n",
    "    balance=BALANCE,\n",
    "    random_state=rs\n",
    ")\n",
    "\n",
    "# Define Random Search Parameters\n",
    "random_search=RandomizedSearchCV(\n",
    "    clf, \n",
    "    param_space, \n",
    "    n_iter=500, \n",
    "    scoring='f1_macro', \n",
    "    n_jobs=-1, \n",
    "    verbose=1,\n",
    "    cv=cv, \n",
    "    random_state=rs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Target/Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input=df.copy()\n",
    "y, X=gen_target_features(\n",
    "    df_input, \n",
    "    TARGET, \n",
    "    token_method=TOKENIZATION_METHOD, \n",
    "    balance=BALANCE,\n",
    "    entity_replacement=ENTITY_REPLACEMENT\n",
    ")\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test=train_test_split(\n",
    "    X, y, \n",
    "    test_size=test_size, \n",
    "    random_state=rs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 30 folds for each of 500 candidates, totalling 15000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  58 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 416 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 858 tasks      | elapsed:   17.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1608 tasks      | elapsed:   31.5s\n",
      "[Parallel(n_jobs=-1)]: Done 2492 tasks      | elapsed:   51.3s\n",
      "[Parallel(n_jobs=-1)]: Done 3480 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4620 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 5896 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 7866 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 9968 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 11852 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 14040 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 15000 out of 15000 | elapsed:  5.3min finished\n"
     ]
    }
   ],
   "source": [
    "result=random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.79\n",
      "Best Hyperparameters: {'clf__C': 33.220820004030706, 'clf__penalty': 'l1', 'clf__solver': 'liblinear'}\n",
      "Pipeline(steps=[('vectorizer',\n",
      "                 CountVectorizer(binary=True, ngram_range=(1, 3))),\n",
      "                ('clf',\n",
      "                 LogisticRegression(C=33.220820004030706, penalty='l1',\n",
      "                                    random_state=5590, solver='liblinear'))])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Score: {result.best_score_:.2}\")\n",
    "print(f\"Best Hyperparameters: {result.best_params_}\")\n",
    "print(result.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.80      0.84      0.82        19\n",
      "     non_lin       0.89      0.74      0.81        23\n",
      "         pos       0.93      0.96      0.95       119\n",
      "\n",
      "    accuracy                           0.91       161\n",
      "   macro avg       0.88      0.85      0.86       161\n",
      "weighted avg       0.91      0.91      0.91       161\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf=result.best_estimator_\n",
    "y_pred=clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t91.3%\n",
      "Precision:\t87.6%\n",
      "Recall:\t\t84.6%\n",
      "F1-Score:\t85.9%\n"
     ]
    }
   ],
   "source": [
    "acc=accuracy_score(y_test, y_pred)\n",
    "prfs_macro=precision_recall_fscore_support(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy:\\t{acc*100:.1f}%\")\n",
    "print(f\"Precision:\\t{prfs_macro[0]*100:.1f}%\")\n",
    "print(f\"Recall:\\t\\t{prfs_macro[1]*100:.1f}%\")\n",
    "print(f\"F1-Score:\\t{prfs_macro[2]*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./../data/Outputs/models/causality_direction/direction.joblib']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Data\n",
    "df_test=pd.DataFrame({\n",
    "    \"target\": y_test,\n",
    "    \"features\": X_test\n",
    "    })\n",
    "\n",
    "file_test_data=f\"test_data_{TARGET}.csv\"\n",
    "path_test_data=os.path.join(directory, file_test_data)\n",
    "df_test.to_csv(path_test_data, index=False)\n",
    "\n",
    "# Model\n",
    "model_name=f\"{TARGET}.joblib\"\n",
    "path=os.path.join(directory, model_name)\n",
    "\n",
    "dump(clf, path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
